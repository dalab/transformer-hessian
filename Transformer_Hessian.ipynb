{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OxBe5uWwKtXo"
   },
   "source": [
    "# What Does it Mean to be a Transformer? - Insights from a Theoretical Hessian Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPLArf0NMGay"
   },
   "source": [
    "## Part 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CvBTz_dPLLak"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import random\n",
    "from typing import Callable, Sequence\n",
    "\n",
    "from curvlinops import GGNLinearOperator, HessianLinearOperator, HutchinsonSquaredFrobeniusNormEstimator\n",
    "import cv2 as cv\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pytorch_lightning import seed_everything\n",
    "import scipy\n",
    "from sklearn import linear_model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tqdm.auto as tqdm\n",
    "\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "V_COLOR ='#7570b3'\n",
    "Q_COLOR ='#1b9e77'\n",
    "\n",
    "SEED=1234\n",
    "N_DIGITS = 5\n",
    "D_VOCAB = 12\n",
    "PLUS_INDEX = 10\n",
    "EQUALS_INDEX = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure plots\n",
    "plt.rcParams['text.usetex'] = True\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['text.latex.preamble']=r\"\\usepackage{amsmath}\"\n",
    "\n",
    "mpl.rcParams['legend.handlelength'] = 1\n",
    "mpl.rcParams['legend.markerscale'] = 0\n",
    "mpl.rcParams['legend.fontsize'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Sil5uniOECa"
   },
   "source": [
    "## Part 2: Data Generator and Loss.\n",
    "This section defines the loss function and the data generator. They are based on the setup from \"Understanding Addition in Transformers\" by Quirke and Barez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lfbCyP_SOA7A"
   },
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "\n",
    "# Calculate the per-token probability by comparing a batch of prediction \"logits\" to answer \"tokens\"\n",
    "def logits_to_tokens_loss(logits: np.array, tokens: np.array):\n",
    "\n",
    "  # Adding 2 five-digit numbers gives a six-digit answer\n",
    "  n_answer_digits = N_DIGITS+1\n",
    "\n",
    "  # The addition answer digit token probabilities\n",
    "  # The \"+1\" below is needed because each answer digit calculations occurs one token before the that answer digit's token is revealed.\n",
    "  ans_logits = logits[:, -(n_answer_digits+1):-1]\n",
    "\n",
    "  # Convert raw score (logits) vector into a probability distribution.\n",
    "  # Emphasize the largest scores and suppress the smaller ones, to make them more distinguishable.\n",
    "  ans_probs = F.log_softmax(ans_logits.to(torch.float64), dim=-1)\n",
    "\n",
    "  max_indices = torch.argmax(ans_probs, dim=-1)\n",
    "\n",
    "  # The addition answer digit tokens\n",
    "  ans_tokens = tokens[:, -(n_answer_digits):]\n",
    "\n",
    "  # Extract values from the ans_probs tensor, based on indices from the ans_tokens tensor\n",
    "  ans_loss = torch.gather(ans_probs, -1, ans_tokens[:, :, None])[..., 0]\n",
    "  # ans_loss = torch.gather(ans_logits.to(torch.float64), -1, ans_tokens[:, :, None])[..., 0]\n",
    "\n",
    "  return ans_loss, max_indices\n",
    "\n",
    "# Calculate loss as negative of average per-token mean probability\n",
    "def loss_fn(ans_loss: np.array):\n",
    "  return -ans_loss.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F3brLMbIOEw3"
   },
   "outputs": [],
   "source": [
    "# Define \"iterator\" data generator function. Invoked using next().\n",
    "# Batch entries are in format XXXXX+YYYYY=ZZZZZZ e.g. 55003+80002=135005\n",
    "# Note that answer has one more digit than the question\n",
    "# Returns characteristics of each batch entry to aid later graphing\n",
    "def data_generator(batch_size: int, n_digits: int, seed: int):\n",
    "    torch.manual_seed(seed)\n",
    "    while True:\n",
    "        #generate a batch of addition questions (answers calculated below)\n",
    "        batch = torch.zeros((batch_size, 3*n_digits+3)).to(torch.int64)\n",
    "        x = torch.randint(0, 10, (batch_size, n_digits))\n",
    "        y = torch.randint(0, 10, (batch_size, n_digits))\n",
    "\n",
    "\n",
    "        # The UseSum9 task is compound and rare (6%) and so the hardest to learn.\n",
    "        # For 20% of batches, we increase the MakeSum9 cases by 20%\n",
    "        # UseSum9 also relies on MakeCarry1 (50%) from previous column.\n",
    "        # So UseSum9 frequency is increased by 20% * 20% * 50% = 2%\n",
    "        if random.randint(1, 5) == 1:\n",
    "          # Flatten x and y to 1D tensors\n",
    "          x_flat = x.view(-1)\n",
    "          y_flat = y.view(-1)\n",
    "\n",
    "          num_elements_to_modify = int(0.20 * x.numel())\n",
    "          indices_to_modify = torch.randperm(x_flat.numel())[:num_elements_to_modify]\n",
    "          if random.randint(1, 2) == 1:\n",
    "            x_flat[indices_to_modify] = 9 - y_flat[indices_to_modify]\n",
    "          else:\n",
    "            y_flat[indices_to_modify] = 9 - x_flat[indices_to_modify]\n",
    "\n",
    "          # Reshape x and y back to its original shape\n",
    "          x = x_flat.view(x.shape)\n",
    "          y = y_flat.view(x.shape)\n",
    "\n",
    "\n",
    "        batch[:, :n_digits] = x\n",
    "        batch[:, n_digits] = PLUS_INDEX\n",
    "        batch[:, 1+n_digits:1+n_digits*2] = y\n",
    "        batch[:, 1+n_digits*2] = EQUALS_INDEX\n",
    "\n",
    "        # These attributes are used for testing the model training progress\n",
    "        base_adds = torch.zeros((batch_size,n_digits)).to(torch.int64)\n",
    "        make_carry1s = torch.zeros((batch_size,n_digits)).to(torch.int64)\n",
    "        sum9s = torch.zeros((batch_size,n_digits)).to(torch.int64)\n",
    "        use_carry1s = torch.zeros((batch_size,n_digits)).to(torch.int64)\n",
    "        use_sum9s = torch.zeros((batch_size,n_digits)).to(torch.int64)\n",
    "\n",
    "        # generate the addition question answers & other info for testing\n",
    "        for i in range(n_digits):\n",
    "            # the column in the test attributes being updated\n",
    "            test_col = n_digits-1-i\n",
    "\n",
    "            base_add = batch[:, n_digits-1-i]+batch[:, 2*n_digits-i]\n",
    "            base_adds[:, test_col] = base_add\n",
    "\n",
    "            sum9 = (base_add == 9)\n",
    "            sum9s[:, test_col] = sum9\n",
    "\n",
    "            if i>0:\n",
    "              use_carry1s[:, test_col] = make_carry1s[:, test_col+1]\n",
    "            use_carry = use_carry1s[:, test_col]\n",
    "\n",
    "            use_sum9s[:, test_col] = sum9 & use_carry;\n",
    "\n",
    "            digit_sum = base_add + use_carry1s[:, test_col]\n",
    "\n",
    "            make_carry = (digit_sum >= 10)\n",
    "            make_carry1s[:, test_col] = make_carry\n",
    "\n",
    "            batch[:, -1-i] = (digit_sum % 10)\n",
    "\n",
    "        # Final (possible) carry to highest digit of the sum\n",
    "        batch[:, -1-n_digits] = make_carry1s[:, 0]\n",
    "\n",
    "        yield batch.cuda(), base_adds.cuda(), make_carry1s.cuda(), sum9s.cuda(), use_carry1s.cuda(), use_sum9s.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.reduction = 'mean'\n",
    "        super(LossModule, self).__init__()\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        per_token_train_losses_raw, _ = logits_to_tokens_loss(outputs, targets)\n",
    "        per_token_train_losses = loss_fn(per_token_train_losses_raw).mean()\n",
    "        return per_token_train_losses\n",
    "\n",
    "class MSELossModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.reduction = 'mean'\n",
    "        super(MSELossModule, self).__init__()\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "\n",
    "        n_answer_digits = N_DIGITS+1\n",
    "\n",
    "        # The addition answer digit token probabilities\n",
    "        # The \"+1\" below is needed because each answer digit calculations occurs one token before the that answer digit's token is revealed.\n",
    "        outputs = outputs[:, -(n_answer_digits+1):-1]\n",
    "        targets = targets[:, -(n_answer_digits+1):-1]\n",
    "        targets = torch.nn.functional.one_hot(targets, D_VOCAB)\n",
    "        return torch.linalg.norm(outputs - targets) ** 2\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Heterogeneity\n",
    "\n",
    "In this section we demonstrate the heterogeneity of the Transformer Hessian and show how softmax influences it. The presented results correspond to figures 1 and 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(SEED)\n",
    "\n",
    "cfg = HookedTransformerConfig(\n",
    "    n_layers = 1,\n",
    "    n_heads = 1,\n",
    "    d_model = 16,\n",
    "    d_head = 16,\n",
    "    d_mlp = 4 * 16,\n",
    "    act_fn = 'relu',\n",
    "    normalization_type = None,\n",
    "    d_vocab=D_VOCAB,\n",
    "    d_vocab_out=D_VOCAB,\n",
    "    n_ctx=3 * N_DIGITS + 3,\n",
    "    init_weights = True,\n",
    "    device=\"cuda\",\n",
    "    seed = SEED,\n",
    ")\n",
    "model = HookedTransformer(cfg)\n",
    "\n",
    "NUM_BATCH = 1 \n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_order = ['embed.W_E', 'pos_embed.W_pos', 'blocks.0.ln1.w', 'blocks.0.ln1.b', 'blocks.0.attn.W_Q', 'blocks.0.attn.W_K', 'blocks.0.attn.W_V',\n",
    "                'blocks.0.attn.W_O', 'blocks.0.ln2.w', 'blocks.0.ln2.b', 'blocks.0.mlp.W_in',  'blocks.0.mlp.b_in', 'blocks.0.mlp.W_out',\n",
    "                'blocks.0.mlp.b_out', 'ln_final.w', 'ln_final.b', 'unembed.W_U', 'unembed.b_U']\n",
    "attention_params_order = ['blocks.0.attn.W_Q', 'blocks.0.attn.W_K', 'blocks.0.attn.W_V', 'blocks.0.attn.W_O']\n",
    "\n",
    "ds = data_generator(BATCH_SIZE, N_DIGITS, SEED)\n",
    "dataset_sample = []\n",
    "for _ in range(NUM_BATCH):\n",
    "  tokens = next(ds)[0]\n",
    "  dataset_sample.append((tokens, tokens))\n",
    "\n",
    "param_dict = {n:p for (n, p) in model.named_parameters()}\n",
    "params = [param_dict[n] for n in params_order if n in param_dict]\n",
    "params_attention = [param_dict[n] for n in attention_params_order if n in param_dict]\n",
    "num_params = sum(p.numel() for p in params)\n",
    "num_params_attention = sum(p.numel() for p in params_attention)\n",
    "num_params_layer_all = [\n",
    "    p.numel() for p in params\n",
    "]\n",
    "num_params_layer_attention = [\n",
    "    p.numel() for p in params_attention\n",
    "]\n",
    "\n",
    "\n",
    "Hessian_linop = HessianLinearOperator(model, LossModule(), params, dataset_sample)\n",
    "Hessian_linop_attention = HessianLinearOperator(model, LossModule(), params_attention, dataset_sample)\n",
    "\n",
    "Hessian_mat = Hessian_linop @ np.eye(num_params).astype(Hessian_linop.dtype)\n",
    "Hessian_mat_attention = Hessian_linop_attention @ np.eye(num_params_attention).astype(Hessian_linop_attention.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices = [Hessian_mat, Hessian_mat_attention]\n",
    "titles = [\"Transformer Block\", \"Self-Attention\"]\n",
    "num_params_layer_group = [num_params_layer_all, num_params_layer_attention]\n",
    "\n",
    "rows, columns = 1, 2\n",
    "img_width = 7\n",
    "\n",
    "plt.rcParams['font.size'] = 20\n",
    "\n",
    "def logabs(mat: np.array, epsilon: float = 1e-6) -> np.array:\n",
    "    return np.log10(np.abs(mat) + epsilon)\n",
    "\n",
    "def plot(\n",
    "    transform: Callable[[np.ndarray], np.ndarray], transform_title: str = None\n",
    "):\n",
    "    \"\"\"Visualize transformed curvature matrices using a shared domain.\n",
    "\n",
    "    Args:\n",
    "        transform: A transformation that will be applied to the matrices. Must\n",
    "            accept a matrix and return a matrix of the same shape.\n",
    "        transform_title: An optional string describing the transformation.\n",
    "            Default: `None` (empty).\n",
    "\n",
    "    Returns:\n",
    "        Figure and axes of the created subplot.\n",
    "    \"\"\"\n",
    "    min_value = min(transform(mat).min() for mat in matrices)\n",
    "    max_value = max(transform(mat).max() for mat in matrices)\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=rows, ncols=columns, figsize=(columns * img_width, rows * img_width)\n",
    "    )\n",
    "\n",
    "    for idx, (ax, mat, title, num_params_layer) in enumerate(zip(axes.flat, matrices, titles, num_params_layer_group)):\n",
    "        ax.set_title(title, pad=20)\n",
    "        kernel = np.ones((5,5),np.float32)/25\n",
    "        dst = cv.filter2D(transform(mat),-1,kernel)\n",
    "        dst = cv.GaussianBlur(transform(mat),(5,5),sigmaX=100, sigmaY=100)\n",
    "        img = ax.imshow(dst, vmin=min_value, vmax=max_value)\n",
    "        ax.axis('off')\n",
    "\n",
    "        # layer structure\n",
    "        for pos in np.cumsum(num_params_layer):\n",
    "            if pos not in [0, num_params]:\n",
    "                style = {\"color\": \"w\", \"lw\": 0.5, \"ls\": \"--\", \"alpha\":0.8}\n",
    "                ax.axhline(y=pos - 1, xmin=0, xmax=num_params - 1, **style)\n",
    "                ax.axvline(x=pos - 1, ymin=0, ymax=num_params - 1, **style)\n",
    "\n",
    "        # colorbar\n",
    "        last = idx == len(matrices) - 1\n",
    "        if last:\n",
    "            cb = fig.colorbar(\n",
    "                img, ax=axes.ravel().tolist(), label=transform_title, shrink=0.8\n",
    "            )\n",
    "            cb.set_label(transform_title, labelpad=20)\n",
    "\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot(logabs, transform_title=\"Logarithmic Absolute Entries\")\n",
    "plt.savefig('./figures/heterogeneity.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute block histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FONTSIZE = 18\n",
    "plt.rcParams['font.size'] = FONTSIZE\n",
    "\n",
    "def plot_hists(matrix1: np.array, matrix2: np.array, key: str, matrix3: np.array = None, matrix4: np.array = None):\n",
    "    plt.close()\n",
    "    def h(vals, color, alpha, ax, b):\n",
    "        logbins = np.logspace(max(-8, np.log10(np.min(vals))),np.log10(np.max(vals)),b)\n",
    "        ax.hist(vals, edgecolor='black', color=color, bins=logbins, alpha=alpha)\n",
    "\n",
    "    if key=='classical':\n",
    "        # Create subplots: 1 row, 2 columns\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(5.5, 4.5), sharex=True)  # 1 row, 2 columns\n",
    "    elif key=='linear':\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9.3, 3), sharey=True)  # 1 row, 2 columns\n",
    "\n",
    "    \n",
    "    # Plot histogram for V block\n",
    "    if matrix3 is not None:\n",
    "        h(matrix3.flatten(), color=V_COLOR, alpha=0.4, ax=ax1, b=50)\n",
    "    \n",
    "    h(matrix1.flatten(), color=V_COLOR, alpha=1, ax=ax1, b=50)\n",
    "    ax1.set_xscale('log')\n",
    "    \n",
    "    # Plot histogram for Q block\n",
    "    if key == 'linear':\n",
    "        bins=50\n",
    "        bins_opaque=30\n",
    "    elif key == 'classical':\n",
    "        bins=30\n",
    "        bins_opaque=-1\n",
    "    if matrix4 is not None:\n",
    "        h(matrix4.flatten(), color=Q_COLOR, alpha=0.4, ax=ax2, b=bins_opaque)\n",
    "    h(matrix2.flatten(), color=Q_COLOR, alpha=1.0, ax=ax2, b=bins)\n",
    "    ax2.set_xscale('log')\n",
    "    \n",
    "    if key == 'classical':\n",
    "        ax2.set_xlabel('Absolute Entries')\n",
    "        fig.supylabel('Frequency', x=0.07, fontsize=FONTSIZE)\n",
    "    elif key == 'linear':\n",
    "        ax1.set_ylabel('Frequency')\n",
    "        fig.supxlabel('Absolute Entries', y=0.17, fontsize=FONTSIZE)\n",
    "    \n",
    "    # Adjust layout to prevent overlapping\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save and show the plot\n",
    "    plt.savefig(f'./figures/histogram_{key}.pdf', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classical self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(SEED)\n",
    "\n",
    "cfg = HookedTransformerConfig(\n",
    "    n_layers = 1,\n",
    "    n_heads = 1,\n",
    "    d_model = 16,\n",
    "    d_head = 16,\n",
    "    d_mlp = 4 * 16,\n",
    "    act_fn = 'relu',\n",
    "    normalization_type = None,\n",
    "    d_vocab=D_VOCAB,\n",
    "    d_vocab_out=D_VOCAB,\n",
    "    n_ctx=3 * N_DIGITS + 3,\n",
    "    init_weights = True,\n",
    "    device=\"cuda\",\n",
    "    seed = SEED,\n",
    ")\n",
    "model = HookedTransformer(cfg)\n",
    "\n",
    "NUM_BATCH = 8\n",
    "BATCH_SIZE = 8\n",
    "ds = data_generator(BATCH_SIZE, N_DIGITS, SEED)\n",
    "dataset_sample = []\n",
    "\n",
    "for _ in range(NUM_BATCH):\n",
    "  tokens = next(ds)[0]\n",
    "  dataset_sample.append((tokens, tokens))\n",
    "\n",
    "param_dict = {n:p for (n, p) in model.named_parameters()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Query histogram\n",
    "q_hessian_linop = HessianLinearOperator(model, LossModule(), [param_dict['blocks.0.attn.W_Q']], dataset_sample)\n",
    "q_hessian_mat = q_hessian_linop @ np.eye(param_dict['blocks.0.attn.W_Q'].numel()).astype(q_hessian_linop.dtype)\n",
    "\n",
    "# Value histogram\n",
    "v_hessian_linop = HessianLinearOperator(model, LossModule(), [param_dict['blocks.0.attn.W_V']], dataset_sample)\n",
    "v_hessian_mat = v_hessian_linop @ np.eye(param_dict['blocks.0.attn.W_V'].numel()).astype(v_hessian_linop.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hists(np.abs(v_hessian_mat), np.abs(q_hessian_mat), 'classical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(SEED)\n",
    "\n",
    "cfg = HookedTransformerConfig(\n",
    "    n_layers = 1,\n",
    "    n_heads = 1,\n",
    "    d_model = 16,\n",
    "    d_head = 16,\n",
    "    d_mlp = 4 * 16,\n",
    "    act_fn = 'relu',\n",
    "    normalization_type = None, #None, #\"LNPre\", # None, #'LN',\n",
    "    # post_embedding_ln=True,\n",
    "    d_vocab=D_VOCAB,\n",
    "    d_vocab_out=D_VOCAB,\n",
    "    n_ctx=3 * N_DIGITS + 3,\n",
    "    init_weights = True,\n",
    "    device=\"cuda\",\n",
    "    seed = SEED,\n",
    "    # init_mode=\"muP\",\n",
    "    linear_attention=True\n",
    ")\n",
    "model = HookedTransformer(cfg)\n",
    "\n",
    "NUM_BATCH = 8\n",
    "BATCH_SIZE = 8\n",
    "ds = data_generator(BATCH_SIZE, N_DIGITS, SEED)\n",
    "dataset_sample = []\n",
    "for _ in range(NUM_BATCH):\n",
    "  tokens = next(ds)[0]\n",
    "  dataset_sample.append((tokens, tokens))\n",
    "\n",
    "param_dict = {n:p for (n, p) in model.named_parameters()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Query histogram\n",
    "q_hessian_linop = HessianLinearOperator(model, LossModule(), [param_dict['blocks.0.attn.W_Q']], dataset_sample)\n",
    "q_hessian_mat_lin = q_hessian_linop @ np.eye(param_dict['blocks.0.attn.W_Q'].numel()).astype(q_hessian_linop.dtype)\n",
    "\n",
    "# Value histogram\n",
    "v_hessian_linop = HessianLinearOperator(model, LossModule(), [param_dict['blocks.0.attn.W_V']], dataset_sample)\n",
    "v_hessian_mat_lin = v_hessian_linop @ np.eye(param_dict['blocks.0.attn.W_V'].numel()).astype(v_hessian_linop.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_hists(np.abs(v_hessian_mat_lin), np.abs(q_hessian_mat_lin), 'linear', np.abs(v_hessian_mat), np.abs(q_hessian_mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Growth Rates in Self-Attention Hessian\n",
    "In this section we show the growth rates associated with the blocks of the self-attention Hessian. The results correspond to the figures 3, 5, 6, and 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate and Save the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_growth_rates(\n",
    "    scale: str,\n",
    "    norm: str,\n",
    "    batch_size: int,\n",
    "    num_batch: int,\n",
    "    seed: int,\n",
    "    n_digits: int,\n",
    "    n_ctx: int,\n",
    "    d_model: int,\n",
    "    d_mlp: int,\n",
    "    repeats: int,\n",
    "    hvp: int,\n",
    "    residual_scaling: float,\n",
    "    num_layers_iter: Sequence,\n",
    "    linear_attention: bool,\n",
    "    loss_name: str,\n",
    "):\n",
    "    if scale == 'log':\n",
    "        sigma = 10 ** np.linspace(-2, 1.0, 20)\n",
    "    elif scale == 'log_short':\n",
    "        sigma = 10 ** np.linspace(-1, 0.0, 20)\n",
    "    elif scale == 'linear':\n",
    "        sigma = np.linspace(0.1, 10.0, 20)\n",
    "    elif scale == 'linear_short':\n",
    "        sigma = np.linspace(0.1, 1.0, 20)\n",
    "    else:\n",
    "        raise ValueError('Scale not known')\n",
    "    if loss_name == 'mse':\n",
    "        loss_module =  MSELossModule()\n",
    "    elif loss_name == 'ce':\n",
    "        loss_module =  LossModule()\n",
    "    else:\n",
    "        raise ValueError('Unknown loss name')\n",
    "    \n",
    "    ds = data_generator(batch_size, n_digits, seed)\n",
    "    dataset_sample = []\n",
    "    for _ in range(num_batch):\n",
    "       tokens = next(ds)[0]\n",
    "       dataset_sample.append((tokens, tokens))\n",
    "    \n",
    "    \n",
    "    for n_layers in num_layers_iter:\n",
    "    \n",
    "        frob_q = {l:{i:[] for i in range(repeats)} for l in range(n_layers)}\n",
    "        frob_v = {l:{i:[] for i in range(repeats)} for l in range(n_layers)}\n",
    "        frob_q_outer = {l:{i:[] for i in range(repeats)} for l in range(n_layers)}\n",
    "        frob_v_outer = {l:{i:[] for i in range(repeats)} for l in range(n_layers)}\n",
    "        frob_q_func = {l:{i:[] for i in range(repeats)} for l in range(n_layers)}\n",
    "        frob_v_func = {l:{i:[] for i in range(repeats)} for l in range(n_layers)}\n",
    "    \n",
    "\n",
    "        for r in tqdm.tqdm(range(repeats)):\n",
    "            cfg = HookedTransformerConfig(\n",
    "                    n_layers = n_layers,\n",
    "                    n_heads = 1,\n",
    "                    d_model = d_model,\n",
    "                    d_head = d_model,\n",
    "                    d_mlp = d_mlp,\n",
    "                    act_fn = 'relu',\n",
    "                    attn_only=False,\n",
    "                    normalization_type = \"LN\" if norm == 'pre' else None,\n",
    "                    post_embedding_ln=False,\n",
    "                    d_vocab=D_VOCAB,\n",
    "                    d_vocab_out=D_VOCAB,\n",
    "                    n_ctx=n_ctx,\n",
    "                    init_weights = True,\n",
    "                    device=\"cuda\",\n",
    "                    seed = seed+r,\n",
    "                    residual_scaling = residual_scaling,\n",
    "                    linear_attention=linear_attention,\n",
    "                )\n",
    "            model = HookedTransformer(cfg)\n",
    "            for s in sigma:\n",
    "                nn.init.normal_(model.embed.W_E, mean=0, std=s)\n",
    "                nn.init.normal_(model.pos_embed.W_pos, mean=0, std=s)\n",
    "        \n",
    "                \n",
    "                param_dict = {n:p for (n, p) in model.named_parameters()}\n",
    "        \n",
    "                for l in range(n_layers):\n",
    "                    Hessian_linop_k = HessianLinearOperator(\n",
    "                        model,\n",
    "                        loss_module,\n",
    "                        [param_dict[f'blocks.{l}.attn.W_Q']],\n",
    "                        dataset_sample,\n",
    "                        check_deterministic=False,\n",
    "                    )\n",
    "                    GGN_linop_k = GGNLinearOperator(\n",
    "                        model,\n",
    "                        loss_module,\n",
    "                        [param_dict[f'blocks.{l}.attn.W_Q']],\n",
    "                        dataset_sample,\n",
    "                        check_deterministic=False\n",
    "                    )\n",
    "                    est = HutchinsonSquaredFrobeniusNormEstimator(Hessian_linop_k)\n",
    "                    frob_q[l][r].append(np.sqrt(np.mean([est.sample() for _ in range(hvp)])))\n",
    "                    est = HutchinsonSquaredFrobeniusNormEstimator(GGN_linop_k)\n",
    "                    frob_q_outer[l][r].append(np.sqrt(np.mean([est.sample() for _ in range(hvp)])))\n",
    "                    est = HutchinsonSquaredFrobeniusNormEstimator(GGN_linop_k - Hessian_linop_k)\n",
    "                    frob_q_func[l][r].append(np.sqrt(np.mean([est.sample() for _ in range(hvp)])))\n",
    "                    \n",
    "                    Hessian_linop_v = HessianLinearOperator(\n",
    "                        model,\n",
    "                        loss_module,\n",
    "                        [param_dict[f'blocks.{l}.attn.W_V']],\n",
    "                        dataset_sample,\n",
    "                        check_deterministic=False\n",
    "                    )\n",
    "                    GGN_linop_v = GGNLinearOperator(\n",
    "                        model,\n",
    "                        loss_module,\n",
    "                        [param_dict[f'blocks.{l}.attn.W_V']],\n",
    "                        dataset_sample,\n",
    "                        check_deterministic=False\n",
    "                    )\n",
    "                    est = HutchinsonSquaredFrobeniusNormEstimator(Hessian_linop_v)\n",
    "                    frob_v[l][r].append(np.sqrt(np.mean([est.sample() for _ in range(hvp)])))\n",
    "                    est = HutchinsonSquaredFrobeniusNormEstimator(GGN_linop_v)\n",
    "                    frob_v_outer[l][r].append(np.sqrt(np.mean([est.sample() for _ in range(hvp)])))\n",
    "                    est = HutchinsonSquaredFrobeniusNormEstimator(GGN_linop_v - Hessian_linop_v)\n",
    "                    frob_v_func[l][r].append(np.sqrt(np.mean([est.sample() for _ in range(hvp)])))\n",
    "        \n",
    "        \n",
    "        for d, name in zip(\n",
    "            [frob_v_outer, frob_v_func, frob_v, frob_q_outer, frob_q_func, frob_q],\n",
    "            ['frob_v_outer', 'frob_v_func', 'frob_v', 'frob_q_outer', 'frob_q_func', 'frob_q']\n",
    "        ):\n",
    "          f_name = f'numerical_results/norm={norm}_num_layers={n_layers}_scale={scale}_residual_scaling={residual_scaling}_linear_attention={linear_attention}_loss={loss_name}_{name}.pickle'\n",
    "          with open(f_name, 'wb') as handle:\n",
    "            pickle.dump(d, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "NUM_BATCH = 1\n",
    "N_CTX = 3 * N_DIGITS + 3\n",
    "D_MODEL = 128 \n",
    "D_MLP = 4 * D_MODEL \n",
    "REPEATS = 20 \n",
    "HVP = 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without layer norm\n",
    "generate_and_save_growth_rates(\n",
    "    scale = 'log',\n",
    "    norm = 'none',\n",
    "    batch_size = BATCH_SIZE,\n",
    "    num_batch = NUM_BATCH,\n",
    "    seed = SEED,\n",
    "    n_digits = N_DIGITS,\n",
    "    n_ctx = N_CTX,\n",
    "    d_model = D_MODEL,\n",
    "    d_mlp = D_MLP,\n",
    "    repeats = REPEATS,\n",
    "    hvp = HVP,\n",
    "    residual_scaling = 1.0,\n",
    "    num_layers_iter = [1, 2, 3, 4, 5],\n",
    "    linear_attention = False,\n",
    "    loss_name = 'ce',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without layer norm but with linear scale\n",
    "generate_and_save_growth_rates(\n",
    "    scale = 'linear',\n",
    "    norm = 'none',\n",
    "    batch_size = BATCH_SIZE,\n",
    "    num_batch = NUM_BATCH,\n",
    "    seed = SEED,\n",
    "    n_digits = N_DIGITS,\n",
    "    n_ctx = N_CTX,\n",
    "    d_model = D_MODEL,\n",
    "    d_mlp = D_MLP,\n",
    "    repeats = REPEATS,\n",
    "    hvp = HVP,\n",
    "    residual_scaling = 1.0,\n",
    "    num_layers_iter = [1],\n",
    "    linear_attention = False,\n",
    "    loss_name = 'ce',\n",
    ")\n",
    "\n",
    "generate_and_save_growth_rates(\n",
    "    scale = 'linear_short',\n",
    "    norm = 'none',\n",
    "    batch_size = BATCH_SIZE,\n",
    "    num_batch = NUM_BATCH,\n",
    "    seed = SEED,\n",
    "    n_digits = N_DIGITS,\n",
    "    n_ctx = N_CTX,\n",
    "    d_model = D_MODEL,\n",
    "    d_mlp = D_MLP,\n",
    "    repeats = REPEATS,\n",
    "    hvp = HVP,\n",
    "    residual_scaling = 1.0,\n",
    "    num_layers_iter = [1],\n",
    "    linear_attention = False,\n",
    "    loss_name = 'ce',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With pre-layer norm\n",
    "generate_and_save_growth_rates(\n",
    "    scale = 'log',\n",
    "    norm = 'pre',\n",
    "    batch_size = BATCH_SIZE,\n",
    "    num_batch = NUM_BATCH,\n",
    "    seed = SEED,\n",
    "    n_digits = N_DIGITS,\n",
    "    n_ctx = N_CTX,\n",
    "    d_model = D_MODEL,\n",
    "    d_mlp = D_MLP,\n",
    "    repeats = REPEATS,\n",
    "    hvp = HVP,\n",
    "    residual_scalings = 1.0,\n",
    "    num_layers_iter = [1],\n",
    "    linear_attention = False,\n",
    "    loss_name = 'ce',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without layer norm and softmax\n",
    "generate_and_save_growth_rates(\n",
    "    scale = 'log_short',\n",
    "    norm = 'none',\n",
    "    batch_size = BATCH_SIZE,\n",
    "    num_batch = NUM_BATCH,\n",
    "    seed = SEED,\n",
    "    n_digits = N_DIGITS,\n",
    "    n_ctx = N_CTX,\n",
    "    d_model = D_MODEL,\n",
    "    d_mlp = D_MLP,\n",
    "    repeats = REPEATS,\n",
    "    hvp = HVP,\n",
    "    residual_scaling = 0.0,\n",
    "    num_layers_iter = [3, 2, 1],\n",
    "    linear_attention = True,\n",
    "    loss_name = 'mse',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 16\n",
    "\n",
    "def get_exponent(title: str, param: str, norm: str|None = None, v: Sequence = [], linear_attention: bool = False, n_layers: int = 1):\n",
    "    if linear_attention:\n",
    "        if norm=='pre':\n",
    "            regr = linear_model.LinearRegression()\n",
    "            regr.fit(\n",
    "                np.expand_dims(np.log(np.array(sigma)), axis=1),\n",
    "                np.expand_dims(np.log(np.mean(np.array([v[r] for r in range(repeats)]), axis=0)), axis=1),\n",
    "            )\n",
    "            return np.round(regr.coef_, 1)[0][0]\n",
    "\n",
    "        return 2 * 3 ** n_layers\n",
    "    \n",
    "    if norm=='pre':\n",
    "        regr = linear_model.LinearRegression()\n",
    "        regr.fit(\n",
    "            np.expand_dims(np.log(np.array(sigma)), axis=1),\n",
    "            np.expand_dims(np.log(np.mean(np.array([v[r] for r in range(repeats)]), axis=0)), axis=1),\n",
    "        )\n",
    "        return np.round(regr.coef_, 1)[0][0]\n",
    "        \n",
    "    if param == 'q':\n",
    "        if '{o}' in title:\n",
    "            return 6\n",
    "        else:\n",
    "            return 5\n",
    "\n",
    "    elif param == 'v':\n",
    "        if '{f}' in title:\n",
    "            return 0\n",
    "        else:\n",
    "            return 2\n",
    "\n",
    "def read_numerical_results(path_base: str, scale: str):\n",
    "    if scale == 'log':\n",
    "        sigma = 10 ** np.linspace(-2, 1.0, 20)\n",
    "    elif scale == 'log_short':\n",
    "        sigma = 10 ** np.linspace(-1, 0.0, 20)\n",
    "    elif scale == 'linear':\n",
    "        sigma = np.linspace(0.1, 10.0, 20)\n",
    "    elif scale == 'linear_short':\n",
    "        sigma = np.linspace(0.1, 1.0, 20)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError('Scale not known')\n",
    "    \n",
    "    with open(f'{path_base}frob_v_outer.pickle', 'rb') as handle:\n",
    "        frob_v_outer = pickle.load(handle)\n",
    "    with open(f'{path_base}frob_v_func.pickle', 'rb') as handle:\n",
    "        frob_v_func = pickle.load(handle)\n",
    "    with open(f'{path_base}frob_v.pickle', 'rb') as handle:\n",
    "        frob_v = pickle.load(handle)\n",
    "    with open(f'{path_base}frob_q_outer.pickle', 'rb') as handle:\n",
    "        frob_q_outer = pickle.load(handle)\n",
    "    with open(f'{path_base}frob_q_func.pickle', 'rb') as handle:\n",
    "        frob_q_func = pickle.load(handle)\n",
    "    with open(f'{path_base}frob_q.pickle', 'rb') as handle:\n",
    "        frob_q = pickle.load(handle)\n",
    "\n",
    "    return sigma, frob_v, frob_q, frob_v_outer, frob_q_outer, frob_v_func, frob_q_func\n",
    "\n",
    "\n",
    "def plot_growth_rate(\n",
    "    sigma: Sequence,\n",
    "    frob_v: Sequence,\n",
    "    frob_q: Sequence,\n",
    "    frob_v_outer: Sequence,\n",
    "    frob_q_outer: Sequence,\n",
    "    frob_v_func: Sequence,\n",
    "    frob_q_func: Sequence,\n",
    "    file_name_base: str,\n",
    "    repeats: int,\n",
    "    norm: str,\n",
    "    scale: str,\n",
    "    linear_attention: bool = False,\n",
    "    n_layers: int = 1\n",
    "):\n",
    "    \n",
    "    fig, axss = plt.subplots(2, 3, figsize=(12, 4), sharex=True)\n",
    "    \n",
    "    frobs_v = [frob_v_outer, frob_v_func, frob_v]\n",
    "    frobs_q = [frob_q_outer, frob_q_func, frob_q]\n",
    "    titles = [r'$\\mathbf{H}_{\\text{o}}$', r'$\\mathbf{H}_{\\text{f}}$', '$\\mathbf{H}$']\n",
    "    \n",
    "    \n",
    "    for k, (axs, p, fs, c) in enumerate(zip(axss, ['v', 'q'], [frobs_v, frobs_q], [V_COLOR, Q_COLOR])):\n",
    "        for j, (ax, v) in enumerate(zip(axs, fs)):\n",
    "    \n",
    "            if not linear_attention and p == 'v' and j == 1 and norm == 'none' and n_layers == 1:\n",
    "                ax.spines['top'].set_visible(False)\n",
    "                ax.spines['right'].set_visible(False)\n",
    "                ax.spines['left'].set_visible(False)\n",
    "                ax.spines['bottom'].set_visible(False)\n",
    "    \n",
    "                ax.xaxis.set_ticks([])  # Remove x-axis ticks\n",
    "                ax.yaxis.set_ticks([])  # Remove y-axis ticks  \n",
    "                ax.set_xticks([], minor=True)\n",
    "                ax.xaxis.set_ticks_position('none') \n",
    "                continue\n",
    "\n",
    "            mi = np.infty\n",
    "            ma = 0\n",
    "            title = titles[j]\n",
    "            \n",
    "            for l in range(n_layers):\n",
    "                mean_to_plot = np.mean(np.array([v[l][r] for r in range(repeats)]), axis=0)\n",
    "                mi = min(mi, np.nanmin(mean_to_plot))\n",
    "                ma = max(ma, np.nanmax(mean_to_plot))\n",
    "\n",
    "                ax.errorbar(\n",
    "                    sigma,\n",
    "                    mean_to_plot,\n",
    "                    scipy.stats.sem(\n",
    "                        np.array([v[l][r] for r in range(repeats)]),\n",
    "                        axis=0),\n",
    "                    color=c,\n",
    "                    alpha=(l+2) / (n_layers+1),\n",
    "                    label = l+1\n",
    "                )\n",
    "            if (p != 'v' or j != 1 or norm == 'pre') and not (linear_attention and j == 1 and n_layers == 1):\n",
    "                exp = get_exponent(title, p, norm, v[l], linear_attention, n_layers)\n",
    "                if linear_attention:\n",
    "                        iter_range = range(-30, 30)\n",
    "                        min_i = -30\n",
    "                        max_i = 30\n",
    "            \n",
    "                        for i in iter_range:\n",
    "                            ax.plot(sigma, [1000 ** i * s**exp for s in sigma], color='gray', linestyle='dashed', alpha=0.2)\n",
    "\n",
    "                        ax.text(sigma[-1] - 0.2, max(10 ** -16, mi), f'$\\sigma^{{{exp}}}$', fontsize=12, alpha=0.8)\n",
    "\n",
    "                elif scale == 'log':\n",
    "                    iter_range = range(-30, 30)\n",
    "                    min_i = -30\n",
    "                    max_i = 30\n",
    "    \n",
    "                    for i in iter_range:\n",
    "                        ax.plot(\n",
    "                            sigma,\n",
    "                            [10 ** i * s**exp for s in sigma],\n",
    "                            color='gray',\n",
    "                            linestyle='dashed',\n",
    "                            alpha=0.2)\n",
    "                    if norm == 'pre':\n",
    "                        ax.text(sigma[0] + 0.003, 0.5 * mi, f'$\\sigma^{{{exp:.1f}}}$', fontsize=12, alpha=0.8)\n",
    "                    else:\n",
    "                        ax.text(0.5 * sigma[-1], mi, f'$\\sigma^{{{exp}}}$', fontsize=12, alpha=0.8)\n",
    "                elif scale in ['linear', 'linear_short']:\n",
    "                    iter_range = range(-30, 5)\n",
    "                    min_i = -20\n",
    "                    max_i = 30\n",
    "    \n",
    "                    for i in iter_range:\n",
    "                        ax.plot(sigma, [2 ** i * s**exp for s in sigma], color='gray', linestyle='dashed', alpha=0.2)\n",
    "                    if norm == 'pre':\n",
    "                        ax.text(sigma[0] + 0.001, 0.5 * mi, f'$\\sigma^{{{exp}}}$', fontsize=12, alpha=0.8)\n",
    "                    else:\n",
    "                        ax.text(sigma[-1]- 1, mi + 0.1 * (ma - mi), f'$\\sigma^{{{exp}}}$', fontsize=12, alpha=0.8)\n",
    "                    \n",
    "            if scale in ['log', 'log_short']:\n",
    "                ax.set_yscale('log')\n",
    "                ax.set_xscale('log')\n",
    "            mi = 0.1 * mi\n",
    "            if scale in ['linear', 'linear_short']:\n",
    "                ma = 1.1 * ma\n",
    "            elif scale == 'log':\n",
    "                ma = 10 * ma\n",
    "\n",
    "            if linear_attention:\n",
    "                ax.set_ylim((max(mi, 10**-17), min(ma, 10**19)))\n",
    "            else:\n",
    "                ax.set_ylim((max(mi, 10**-15), ma))\n",
    "\n",
    "            \n",
    "            if j == 0:\n",
    "                handles, labels = ax.get_legend_handles_labels()\n",
    "                handles = [h[0] for h in handles]\n",
    "                if n_layers > 1:\n",
    "                    ax.legend(handles, labels, loc='upper left', title='Layer:', title_fontsize=10)\n",
    "            if k == 1:\n",
    "                ax.set_xlabel(r'$\\sigma$', labelpad=-5)\n",
    "                if scale == 'log':\n",
    "                    ax.set_xticks([0.01, 0.1, 1, 10])\n",
    "                elif scale == 'linear_short':\n",
    "                    ax.set_xticks([0.01, 0.5, 1.0])\n",
    "    \n",
    "            if k == 0 or (p == 'q' and j == 1 and norm == 'none' and n_layers == 1):\n",
    "                ax.set_title(title)\n",
    "            ax.axvline(x=1, color='black', linestyle='dotted', linewidth=1, alpha = 1.0)\n",
    "            ax.set_xlim((sigma[0], sigma[-1]))\n",
    "\n",
    "            if linear_attention:\n",
    "                ax.set_xticks([0.1, 0.2, 0.5, 1.0])\n",
    "                ax.set_xticklabels([0.1, 0.2, 0.5, 1.0])\n",
    "                ax.xaxis.set_ticks([0.1, 0.2, 0.5, 1.0])\n",
    "                ax.xaxis.set_ticks_position('none') \n",
    "                ax.tick_params(\n",
    "                    axis='x',          # changes apply to the x-axis\n",
    "                    which='minor',      # both major and minor ticks are affected\n",
    "                    bottom=False,      # ticks along the bottom edge are off\n",
    "                    top=False,         # ticks along the top edge are off\n",
    "                    labelbottom=False)\n",
    "            elif scale == 'linear_short':\n",
    "                ax.set_xticks([0.01, 0.5, 1.0])\n",
    "                ax.xaxis.set_ticks([0.01, 0.5, 1.0])\n",
    "            elif scale == 'linear':\n",
    "                ax.set_xticks([0, 1, 4, 7, 10])\n",
    "                ax.xaxis.set_ticks([0, 1, 4, 7, 10])\n",
    "        \n",
    "    \n",
    "    fig.set_tight_layout(False)\n",
    "    fig.supylabel(r'Block $\\|\\cdot\\|_\\text{F}$', x=0.04)\n",
    "    plt.subplots_adjust(wspace=0.25)\n",
    "    plt.savefig(f\"figures/{file_name_base}_all.pdf\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multilayer\n",
    "NORM = 'none'\n",
    "SCALE = 'log'\n",
    "RESIDUAL_SCALING = 1.0\n",
    "LINEAR_ATTENTION = False\n",
    "LOSS_NAME = 'ce'\n",
    "REPEATS = 20\n",
    "\n",
    "for n_layers in [1, 2, 3, 4, 5]:\n",
    "\n",
    "    file_name_base = f'norm={NORM}_num_layers={n_layers}_scale={SCALE}_residual_scaling={RESIDUAL_SCALING}_linear_attention={LINEAR_ATTENTION}_loss={LOSS_NAME}_'\n",
    "    path_base = f'numerical_results/{file_name_base}'\n",
    "    \n",
    "    sigma, frob_v, frob_q, frob_v_outer, frob_q_outer, frob_v_func, frob_q_func = read_numerical_results(path_base, SCALE)\n",
    "    plot_growth_rate(sigma, frob_v, frob_q, frob_v_outer, frob_q_outer, frob_v_func, frob_q_func, file_name_base, REPEATS, NORM, SCALE, LINEAR_ATTENTION, n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear scale\n",
    "NORM = 'none'\n",
    "RESIDUAL_SCALING = 1.0\n",
    "LINEAR_ATTENTION = False\n",
    "LOSS_NAME = 'ce'\n",
    "REPEATS = 20\n",
    "N_LAYERS = 1\n",
    "\n",
    "for scale in ['linear', 'linear_short']:\n",
    "\n",
    "    file_name_base = f'norm={NORM}_num_layers={N_LAYERS}_scale={scale}_residual_scaling={RESIDUAL_SCALING}_linear_attention={LINEAR_ATTENTION}_loss={LOSS_NAME}_'\n",
    "    path_base = f'numerical_results/{file_name_base}'\n",
    "    \n",
    "    \n",
    "    sigma, frob_v, frob_q, frob_v_outer, frob_q_outer, frob_v_func, frob_q_func = read_numerical_results(path_base, scale)\n",
    "    plot_growth_rate(sigma, frob_v, frob_q, frob_v_outer, frob_q_outer, frob_v_func, frob_q_func, file_name_base, REPEATS, NORM, scale, LINEAR_ATTENTION, N_LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single layer pre-norm\n",
    "REPEATS = 20\n",
    "NORM = 'pre'\n",
    "SCALE = 'log'\n",
    "RESIDUAL_SCALING = 1.0\n",
    "N_LAYERS = 1\n",
    "LINEAR_ATTENTION = False\n",
    "LOSS_NAME = 'ce'\n",
    "\n",
    "file_name_base = f'norm={NORM}_num_layers={N_LAYERS}_scale={SCALE}_residual_scaling={RESIDUAL_SCALING}_linear_attention={LINEAR_ATTENTION}_loss={LOSS_NAME}_'\n",
    "path_base = f'numerical_results/{file_name_base}'\n",
    "    \n",
    "sigma, frob_v, frob_q, frob_v_outer, frob_q_outer, frob_v_func, frob_q_func = read_numerical_results(path_base, SCALE)\n",
    "plot_growth_rate(sigma, frob_v, frob_q, frob_v_outer, frob_q_outer, frob_v_func, frob_q_func, file_name_base, REPEATS, NORM, SCALE, LINEAR_ATTENTION, N_LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multilayer linear\n",
    "REPEATS = 20\n",
    "NORM = 'none'\n",
    "SCALE = 'log_short'\n",
    "RESIDUAL_SCALING = 0.0\n",
    "LINEAR_ATTENTION = True\n",
    "LOSS_NAME = 'mse'\n",
    "\n",
    "for n_layers in [1, 2, 3]:\n",
    "\n",
    "    file_name_base = f'norm={NORM}_num_layers={n_layers}_scale={SCALE}_residual_scaling={RESIDUAL_SCALING}_linear_attention={LINEAR_ATTENTION}_loss={LOSS_NAME}_'\n",
    "    path_base = f'numerical_results/{file_name_base}'\n",
    "    \n",
    "    sigma, frob_v, frob_q, frob_v_outer, frob_q_outer, frob_v_func, frob_q_func = read_numerical_results(path_base, SCALE)\n",
    "    plot_growth_rate(sigma, frob_v, frob_q, frob_v_outer, frob_q_outer, frob_v_func, frob_q_func, file_name_base, REPEATS, NORM, SCALE, LINEAR_ATTENTION, n_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Growth Rates in MLP Hessian\n",
    "In this section we show the growth rates associated with the layers of the MLP Hessian. The results correspond to the figure 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPNoActivations(nn.Module):\n",
    "    def __init__(self, hidden_size, vocab_size, num_layers, device='cuda'):\n",
    "        super(MLPNoActivations, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb = nn.Embedding(vocab_size, hidden_size, device=device)\n",
    "        self.layers = nn.Sequential(*[\n",
    "            nn.Linear(\n",
    "                hidden_size,\n",
    "                hidden_size,\n",
    "                device=device,\n",
    "                bias=False\n",
    "            ) for _ in range(num_layers-1)\n",
    "        ] + [\n",
    "            nn.Linear(\n",
    "                hidden_size,\n",
    "                vocab_size,\n",
    "                device=device,\n",
    "                bias=False\n",
    "            )\n",
    "        ])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        BATCH_SIZE, seq_len, emb_dim = x.shape\n",
    "        x = x.reshape((BATCH_SIZE * seq_len, emb_dim))\n",
    "        x = self.layers(x)\n",
    "        x = x.reshape((BATCH_SIZE, seq_len, self.vocab_size))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=64\n",
    "NUM_BATCH = 1\n",
    "D_MODEL = 128 \n",
    "REPEATS=20 \n",
    "HVP=20 \n",
    "num_layers_iter = [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_module =  MSELossModule()\n",
    "sigma = 10 ** np.linspace(-2, 1.0, 20)\n",
    "\n",
    "\n",
    "\n",
    "ds = data_generator(BATCH_SIZE, N_DIGITS, SEED)\n",
    "dataset_sample = []\n",
    "for _ in range(NUM_BATCH):\n",
    "   tokens = next(ds)[0]\n",
    "   dataset_sample.append((tokens, tokens))\n",
    "\n",
    "\n",
    "for n_layers in num_layers_iter:\n",
    "\n",
    "    frob_mlp = {l: {i:[] for i in range(REPEATS)} for l in range(n_layers)}\n",
    "    frob_mlp_outer = {l:{i:[] for i in range(REPEATS)} for l in range(n_layers)}\n",
    "    frob_mlp_func = {l:{i:[] for i in range(REPEATS)} for l in range(n_layers)}\n",
    "\n",
    "\n",
    "    for r in tqdm.tqdm(range(REPEATS)):\n",
    "        model = MLPNoActivations(\n",
    "            num_layers = n_layers,\n",
    "            hidden_size = D_MODEL,\n",
    "            vocab_size=D_VOCAB,\n",
    "        )\n",
    "        for s in sigma:\n",
    "            nn.init.normal_(model.emb.weight, mean=0, std=s)        \n",
    "            \n",
    "            param_dict = {n:p for (n, p) in model.named_parameters()}\n",
    "    \n",
    "            for l in range(n_layers):\n",
    "                Hessian_linop_mlp = HessianLinearOperator(\n",
    "                    model,\n",
    "                    loss_module,\n",
    "                    [param_dict[f'layers.{l}.weight']],\n",
    "                    dataset_sample,\n",
    "                    check_deterministic=False,\n",
    "                )\n",
    "                est = HutchinsonSquaredFrobeniusNormEstimator(Hessian_linop_mlp)\n",
    "                frob_mlp[l][r].append(np.sqrt(np.mean([est.sample() for _ in range(HVP)])))\n",
    "\n",
    "    \n",
    "    for d, name in zip([frob_mlp], ['frob_mlp']):\n",
    "      f_name = f'numerical_results/mlp_num_layers={n_layers}_{name}.pickle'\n",
    "      with open(f_name, 'wb') as handle:\n",
    "        pickle.dump(d, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_layers in num_layers_iter:\n",
    "\n",
    "    base_name = f'mlp_num_layers={n_layers}_{name}'\n",
    "    f_name = f'numerical_results/{base_name}.pickle'\n",
    "\n",
    "    sigma = 10 ** np.linspace(-2, 1.0, 20)\n",
    "    \n",
    "    with open(f_name, 'rb') as handle:\n",
    "        frob_mlp = pickle.load(handle)    \n",
    "    \n",
    "    \n",
    "    fig, axs = plt.subplots(1, 1, figsize=(4, 2), sharex=True)  # 1 row, 3 columns\n",
    "    \n",
    "    frobs_mlp = [frob_mlp]\n",
    "    titles = ['$\\mathbf{H}$']\n",
    "    \n",
    "    if not isinstance(axs, list):\n",
    "        axs = [axs]\n",
    "    \n",
    "    c = 'blue'\n",
    "    \n",
    "    for k, (ax, v, t) in enumerate(zip(axs, frobs_mlp, titles)):\n",
    "    \n",
    "            mi = np.infty\n",
    "            ma = 0\n",
    "    \n",
    "            \n",
    "            for l in range(n_layers):\n",
    "                mean_to_plot = np.mean(np.array([v[l][r] for r in range(repeats)]), axis=0)\n",
    "                mi = min(mi, np.nanmin(mean_to_plot))\n",
    "                ma = max(ma, np.nanmax(mean_to_plot))\n",
    "                \n",
    "                ax.errorbar(\n",
    "                    sigma,\n",
    "                    mean_to_plot,\n",
    "                    scipy.stats.sem(\n",
    "                        np.array([v[l][r] for r in range(repeats)]),\n",
    "                        axis=0),\n",
    "                    color=c,\n",
    "                    alpha=(l+2) / (n_layers+1),\n",
    "                    label = l+1\n",
    "                )\n",
    "            exp = 2\n",
    "            iter_range = range(-30, 30)\n",
    "\n",
    "            for i in iter_range:\n",
    "                ax.plot(sigma, [10 ** i * s**exp for s in sigma], color='gray', linestyle='dashed', alpha=0.2)\n",
    "\n",
    "            ax.text(sigma[-1] - 5, max(10 ** -16, mi), f'$\\sigma^{{{exp}}}$', fontsize=12, alpha=0.8)\n",
    "                    \n",
    "            ax.set_yscale('log')\n",
    "            ax.set_xscale('log')\n",
    "            mi = 0.1 * mi\n",
    "            ma = 10 * ma\n",
    "            ax.set_ylim((max(mi, 10**-17), min(ma, 10**19)))\n",
    "            \n",
    "    \n",
    "            handles, labels = ax.get_legend_handles_labels()\n",
    "            handles = [h[0] for h in handles]\n",
    "            ax.legend(handles, labels, loc='upper left', title='Layer:', title_fontsize=10)\n",
    "            \n",
    "            ax.set_xlabel(r'$\\sigma$', labelpad=-5)\n",
    "    \n",
    "    \n",
    "            ax.set_title(t)\n",
    "            \n",
    "            ax.axvline(x=1, color='black', linestyle='dotted', linewidth=1, alpha = 1.0)\n",
    "            ax.set_xlim((sigma[0], sigma[-1]))\n",
    "    \n",
    "            ax.set_xticks([0.01, 0.1, 1.0, 10.0])\n",
    "            ax.set_xticklabels([0.01, 0.1, 1.0, 10.0])\n",
    "            ax.xaxis.set_ticks([0.01, 0.1, 1.0, 10.0])\n",
    "            ax.xaxis.set_ticks_position('none') \n",
    "            ax.tick_params(\n",
    "                axis='x',          # changes apply to the x-axis\n",
    "                which='minor',      # both major and minor ticks are affected\n",
    "                bottom=False,      # ticks along the bottom edge are off\n",
    "                top=False,         # ticks along the top edge are off\n",
    "                labelbottom=False)\n",
    "        \n",
    "    \n",
    "    fig.set_tight_layout(False)\n",
    "    fig.supylabel(r'Block $\\|\\cdot\\|_\\text{F}$', x=-0.05)\n",
    "    plt.subplots_adjust(wspace=0.25)\n",
    "    plt.savefig(f\"figures/{base_name}.pdf\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "EPLArf0NMGay",
    "d1RSXtlXfaBo",
    "B4ECsPk7Y025",
    "cQmGSsTlYYV3",
    "_K3YyDytmKHx"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (weronika-mt5)",
   "language": "python",
   "name": "weronika-mt5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
